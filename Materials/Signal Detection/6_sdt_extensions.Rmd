---
title     : "<small>Mathematical Modelling</small>"
subtitle  : "Signal Detection Model Extensions"
author    : "<br />`r paste(params$author, collapse = ' & ')`"
date      : "<small>`r params$date`</small>"

output:
  xaringan::moon_reader:
    lib_dir: libs
    self_contained: true
    chakra: libs/remark-latest.min.js
    css: ["src/xaringan-themer.css", "src/slides.css"]
    nature:
      countIncrementalSlides: false
      highlightLines: true
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
---
exclude: true

```{r setup, include=FALSE}
library("knitr")
options(htmltools.dir.version = FALSE)
opts_chunk$set(echo = FALSE, fig.align = "center")

# remotes::install_github("gadenbuie/xaringanExtra")
library("xaringanExtra")
library("xaringanthemer")

my_colors <- c("#bc0031", "#d67f42")

library("dplyr")
library("ggplot2")
library("viridis")
library("papaja")
```

```{r extras-styling, include = FALSE}
use_xaringan_extra(c("tile_view", "clipboard")) #, "broadcast", "webcam", "animate_css"

# style_mono_light("#32475b")
style_mono_accent(
  base_color = my_colors[1]
  # , title_slide_background_image = "src/uva.svg"
  , header_font_google = google_font("Poppins")
  , header_h1_font_size = "36pt"
  , text_font_google = google_font("Open Sans")
  , text_font_size = "22pt"
  , text_color = "#3a3a3a"
  , outfile = "src/xaringan-themer.css"
)
```

```{r}
add_overlay <- function(..., label = NULL, label_style = NULL) {
  el <- list(...)
  
  y <- '<div id="overlay-highlight"'
  if(length(el) > 0) {
    y <- c(y, 'style="', glue::glue('{names(el)}:{el};'))
  }
  y <- c(y, '">')
  
  if(!is.null(label)) {
    y <- c(y, glue::glue('<span" class="vertical-center"" style="{label_style}">{label}</span>'))
  }
  
  knitr::asis_output(glue::glue_collapse(c(y, "</div>")))
}
```


---
class: my-one-page-font

# Agenda

1. Improving optimization
  - Transformation of $d'$
  - Convergence
  - Nested optimization
2. Extensions of SDT models
  - Confidence ratings
  - Unequal variance
3. Dealing with a hierarchical data structure
  - Fitting models to aggregated data
  - Fitting models to individual data
  - Accounting for the hierarchical structure
  
---
layout: false
class: inverse, middle, center

# Improving optimization

```{r, out.width = "50%", out.extra = 'id="zoom-margin"'}
include_graphics("src/improvement.jpg")
```

---
class: my-one-page-font

### Transformation of $d'$

- The default algorithm in optim() assumes that parameters can take on any real value.
- For the signal detection model, $c \in (-\infty, \infty),$ $d' \in (0, \infty).$

--

- Solution: Transform the parameter $d'$ in the function, for example using $\exp(x)$ transformation.

```{r, fig.asp = .6}
par(mar = c(3,3,1,1), mgp = c(2, .7, 0), cex = 1.5)
x <- seq(-3, 1, .01)
d <- exp(x)

plot(x, d, type = "l", ylab = "d'", lwd = 3, col = "slateblue")
```

---
class: my-one-page-font

### Transformation of $d'$ in R

```{r} 
y <- c("hit" = 75, "fa" = 30)
n <- c("signal" = 100, "noise" = 50)
```

```{r echo = T, tidy = FALSE}
sdtm <- function(par, y, n) { #par = c(d', c) y = c(hit, fa)
  # transform d' to parameter space 
  {{dprime <- exp(par[1])}}
  # no transformation needed for criterion
  {{crit <- par[2]}}
  p1 <- 1 - pnorm(crit - dprime)
  p2 <- 1 - pnorm(crit)
  -2 * (dbinom(y["hit"], n["signal"], p1, log = TRUE) +
    dbinom(y["fa"], n["noise"], p2, log = TRUE))
}
out <- optim(par = c(d = 0, c = 0), fn = sdtm, y = y, n = n)
out$par
```

--

```{r echo = T, tidy = FALSE}
c(exp(out$par[1]), out$par[2]) ## transform back
```

---

### Convergence

Did the optimization algorithm reach successful completion?

---
class: my-one-page-font

### Convergence of `optim()`

> *convergence:*
> An integer code. 0 indicates successful completion (which is always the case for "SANN" and "Brent"). Possible error codes are

> 1: indicates that the iteration limit maxit had been reached.

> 10: indicates degeneracy of the Nelder-Mead simplex.

> 51: indicates a warning from the "L-BFGS-B" method; see component message for further details.

> 52: indicates an error from the "L-BFGS-B" method; see component message for further details.

---
class: my-one-page-font

### Checking convergence

$$
h = \sum_{i = 1}^n (y_i - \theta_i)^2
$$

--

```{r echo = T, warning=F}
y <- 1:20

h <- function(theta, y) sum((theta - y)^2)
par <- rep(10, 20) #starting values
optim(par, h, y = y)$convergence
```

--

```{r echo = T}
head(optim(par, h, y = y)$par)
optim(par, h, y = y)$value
```

---
class: my-one-page-font

### Nested optimization

- Frame the analysis so that it involves several separate optimizations with a smaller number of parameters each.
- For the function $h$: Estimate of $\theta_1$ only dependent on $y_1$, not the other observations.

--

```{r echo = T}
par.est <- rep(0,20) #reserve space
min <- 0
for (i in 1:20){
est <- optimize(h, interval = c(-100, 100), y = y[i])
min <- min + est$objective
par.est[i] <- est$minimum #store the estimates
}
min; head(par.est)
```


---
class: my-one-page-font

### Nested optimization, an example

```{r, results='asis'}
rew <- c(10, 7, 5, 3, 1)
Reward1 <- paste(rew, "cents")
Reward2 <- paste(rev(rew), "cents")
Hit <- c(404, 348, 287, 251, 148)
Miss <- c(96, 152, 213, 249, 352)
FA <- c(301, 235, 183, 102, 20)
CR <- c(199, 265, 317, 398, 480)

tab <- cbind(Reward1, Reward2, Hit, Miss, FA, CR)
rownames(tab) <- paste("Cond", LETTERS[1:5])
colnames(tab)[1:2] <- c("Reward signal trial", "Reward noise trial")
knitr::kable(tab)
```

```{r}
y <- cbind(Hit, FA)
n <- c(rowSums(cbind(Hit, Miss))[1], rowSums(cbind(FA, CR))[1])
```

---
class: my-one-page-font

### Nested optimization, an example

- Optimization of parameters specific to conditions, $b_A$, $b_B$, ..., $b_E$ is nested within parameters common across all conditions, $d$.

--

**Steps:**

1. Assume the true value of $d$.
2. Then $b_A$ only depends on condition A, $b_B$ only on condition B, etc. 
3. Compute the likelihood for $b$ in a single condition given a fixed value of $d$.
4. Make a function that separately optimizes $b$ for each condition.
5. Optimize that function for $d$.

---
class: my-one-page-font

### Nested optimization, an example

```{r echo = T}
#negative log likelihood for high-threshold model
nll.ht.given.d <- function(b, y, n, d){ #y = c(hit, fa) for one cond
p1 <- d + (1 - d) * b # probability of a hit
p2 <- b # probability of a false alarm
return(-sum(dbinom(y[1], n[1], p1, log = T)
            , dbinom(y[2], n[2], p2, log = T)))
}
```

--

```{r echo = F, eval = F}
nll.ht <- function(d, y, n){
optimize(nll.ht.given.d, interval = c(0, 1), y = y[1,], n = n, d = d)$objective +
optimize(nll.ht.given.d, interval = c(0, 1), y = y[2,], n = n, d = d)$objective +
optimize(nll.ht.given.d, interval = c(0, 1), y = y[3,], n = n, d = d)$objective +
optimize(nll.ht.given.d, interval = c(0, 1), y = y[4,], n = n, d = d)$objective +
optimize(nll.ht.given.d, interval = c(0, 1), y = y[5,], n = n, d = d)$objective
}
```

```{r echo = T}
nll.ht <- function(d, y, n) {
  nll.ht.opt <- function(y_i) {
    res <- optimize(nll.ht.given.d, interval = c(0, 1), y = y_i, n = n, d = d)
    res$objective
  }
  nll.ht.opt(y[1, ]) + nll.ht.opt(y[2, ]) + nll.ht.opt(y[3, ]) +
    nll.ht.opt(y[4, ]) + nll.ht.opt(y[5, ])
}
```


--

```{r echo = T}
g <- optimize(nll.ht, interval = c(0, 1), y = y, n = n)
```

---
class: my-one-page-font

### Nested optimization, output

```{r echo = T}
g$minimum; g$objective
```

--

```{r echo = T}
optimize(nll.ht.given.d
         , interval = c(0, 1)
         , y = y[1,]
         , n = n
         , d = g$minimum)$minimum
```

---
layout: false
class: inverse, middle, center


# Extensions to the SDT model

---
layout: false
class: inverse, middle, center

# Confidence ratings

```{r, out.width = "50%", out.extra = 'id="zoom-margin"'}
include_graphics("src/confidence.jpg")
```

---

### Confidence ratings

- So far we have focused on models for data with two response options (Binomial models).
- How about more than two options?
--

- Confidence tasks: Participants indicate confidence in their judgments by choosing an option such as "I have low confidence."
- For confidence ratings we need to use a multinomial model (>2 responses).
--

- Q: Any idea how confidence ratings could be implemented as an SDT model?

---
class: my-one-page-font

### Confidence ratings

```{r, out.width = "140%", out.extra = 'id="zoom-margin"'}
include_graphics("src/confidencesdt.png")
```

--

- For $K$ rating options there are $k - 1$ criteria.

---
class: my-one-page-font

### Confidence ratings

```{r, out.width = "60%", out.extra = 'id="zoom-margin"'}
include_graphics("src/confidencescaling.png")
```

.footnote[Selker, van den Bergh, <br> Criss, & Wagenmakers, 2019]

---
layout: false
class: inverse, middle, center

# Unequal variance extension

```{r, out.width = "50%", out.extra = 'id="zoom-margin"'}
include_graphics("src/unequal.jpg")
```

---
class: my-one-page-font

### Equal variance assumption

- In the standard SDT model the variances for the noise and signal distributions are fixed to 1.
--

- Consequence for ROC curves:

```{r, sdt-sensitivity, out.extra = 'id="zoom-margin"', fig.width = 5, fig.asp = 1}
isosensitivity_sdt <- data.frame(
  dprime = rep(seq(0, 1.5, length.out = 6), each = 601)
  , crit = seq(-3, 3, .01)
) %>% mutate(
  hits = 1 - pnorm(crit - dprime)
  , fa = 1 - pnorm(crit)
)

roc_plot <- list(
  geom_abline(slope = 1, intercept = 0, color = grey(0.5), linetype = "dotted", size = 1.25)
  , geom_line(size = 1.25)
  , scale_x_continuous(breaks = c(0, .5, 1))
  , scale_y_continuous(breaks = c(0, .5, 1))
  , scale_color_viridis()
  , coord_fixed()
  , labs(
    x = bquote("False alarm rate")
    , y = bquote("Hit rate")
  )
  , theme_apa(base_size = 24, box = TRUE)
  , theme(
    legend.margin = margin(l = 0, r = 12)
    , legend.title.align = 0.5
    , axis.ticks.length = unit(rel(10), "pt")
    , axis.title.x = element_text(margin = margin(t = 20))
    , axis.title.y = element_text(margin = margin(r = 20))
    , plot.margin = margin(b = 100)
  )
)

sdt_sensitivity_plot <- ggplot(isosensitivity_sdt, aes(x = fa, y = hits, group = dprime, color = dprime)) +
  roc_plot +
  guides(color = guide_colorbar(title = expression("d'"), barwidth = 1.5, barheight = 10))

sdt_sensitivity_plot
```

---
class: my-one-page-font

### Unequal variance

- Equal variance assumption can be relaxed:

$$
S \sim
\begin{cases}
\mbox{Normal}(\mu = d', \sigma^2), \qquad \text{for signal-present trials,}\\
\mbox{Normal}(\mu = 0, 1), \qquad \text{for signal-absent trials.}
\end{cases}
$$
--

- Consequence for ROC curves:

```{r, uvsd-sensitivity, out.extra = 'id="zoom-margin"', fig.width = 5, fig.asp = 1}
isosensitivity_sdt <- data.frame(
  dprime = rep(c(0.5, 1, 2), each = 601)
  , crit = seq(-3, 3, .01)
  , s = rep(seq(1, 2, length.out = 3), each = 601)
) %>% mutate(
  hits = 1 - pnorm((crit - dprime)/s)
  , fa = 1 - pnorm(crit)
)

roc_plot <- list(
  geom_abline(slope = 1, intercept = 0, color = grey(0.5), linetype = "dotted", size = 1.25)
  , geom_line(size = 1.25)
  , scale_x_continuous(breaks = c(0, .5, 1))
  , scale_y_continuous(breaks = c(0, .5, 1))
  , scale_color_viridis()
  , coord_fixed()
  , labs(
    x = bquote("False alarm rate")
    , y = bquote("Hit rate")
  )
  , theme_apa(base_size = 24, box = TRUE)
  , theme(
    legend.margin = margin(l = 0, r = 12)
    , legend.title.align = 0.5
    , axis.ticks.length = unit(rel(10), "pt")
    , axis.title.x = element_text(margin = margin(t = 20))
    , axis.title.y = element_text(margin = margin(r = 20))
    , plot.margin = margin(b = 100)
  )
)

sdt_sensitivity_plot <- ggplot(isosensitivity_sdt, aes(x = fa, y = hits, group = dprime, color = dprime)) +
  roc_plot +
  guides(color = guide_colorbar(title = expression("d'"), barwidth = 1.5, barheight = 10))

sdt_sensitivity_plot
```

---
class: my-one-page-font

### The unequal variance signal detection model (UVSD)

```{r echo = F, fig.asp = .6, fig.width=7}
par(mar = c(3,3,1,1), mgp = c(2,.7,0), cex = 1.5)
x <- seq(-3, 8, .01)
y.noise <- dnorm(x)
y.signal <- dnorm(x, 3.5, sd = 1.5)

plot(x, y.noise
     , type = "l", lwd = 4
     , xlim = range(x)
     , ylim = c(0, .45)
     , frame.plot = F
     , ylab = "Density"
     , xlab = "Sensory Strength"
     , cex.lab = 1.3
     )
lines(x, y.signal, col = "firebrick4", lwd = 4)
lines(rep(1, 2), c(0, .35), lwd = 4, col = "darkgreen")

plotrix::plotCI(3.5/2, .42, ui = 3.5, li = 0, err = "x", add = T, pch = NA, lwd = 3, col = "darkgray")
text(3.5/2, .45, "d'", cex = 1.3)
text(1.4, .03, "c", cex = 1.5)
plotrix::plotCI(3.5, .1, ui = 5, li = 2, err = "x", add = T, pch = NA, lwd = 3, col = "darkgray")
text(3.5, .13, expression(sigma), cex = 1.3)

```

--

$$
\begin{align}
p_1 = p(\text{"Signal"}~|~\text{Signal}) & = 1 - \Phi\Big(\frac{c - d'}{\sigma}\Big), \\
p_2 = p(\text{"Signal"}~|~\text{Noise}) & = 1 - \Phi(c)
\end{align}
$$

---

### UVSD and estimation

Identifiability?

--

For standard signal detextion experiments or memory experiments the UVSD model is not identified.

- Three parameters ( $d'$, $c$, $\sigma$).
- Two independent observations (hits and false alarms).

---

### Let's get some more data

.pull-left-50[

```{r, out.width = "50%", out.extra = 'id="zoom-margin"'}
include_graphics("src/SP.png")
```

]

.pull-right-50[


- Is there a fixed capacity limit to visual working memory?
- Manipulated: Set size (number of items to remember): 2, 5, or 8

.footnote[Rouder et al., 2008]

]

---

### Let's get some more data

```{r}
# MLE Rouder et al (2008) PNAS
cd <- read.table(file = "src/rouder08-data-0.5.dat")
group_data <- apply(cd, 2, sum)

cond2 <- c(hits = group_data[1], fa = group_data[3])
cond5 <- c(hits = group_data[5], fa = group_data[7])
cond8 <- c(hits = group_data[9], fa = group_data[11])
y <- cbind(cond2, cond5, cond8)
rownames(y) <- c("hits", "fas")

n <- c(old = sum(group_data[1:2]), new = sum(group_data[3:4]))

knitr::kable(y)
```

---

## Fitting the UVSD model

- Three conditions, 6 independent data points.
- How many parameters if we let everything vary across conditions?
--

- Restrict the model: Assume only $d'$ is affected.

---

### UVSD for change detection

```{r, echo = F, fig.asp = .6}
par(mar=c(4,3,1,1), mgp = c(2, .7, 0), cex = 1.5)
curve(expr = dnorm(x, 0, 1), from = -3, to = 6, xlab="Strength of evidence for 'change'", ylab="", lwd=2)

curve(expr = dnorm(x, 1, 1.2), col="tomato", from = -3, to = 6, lwd=2, add = T)
curve(expr = dnorm(x, 2, 1.2), col="forestgreen", from = -3, to = 6, lwd=2, add = T)
curve(expr = dnorm(x, 3, 1.2), col="dodgerblue", from = -3, to = 6, lwd=2, add = T)

legend("topleft", legend = c(2,5,8), lty = 1, col = c("dodgerblue", "forestgreen","tomato"), title = "N", lwd=2, bty='n')

```

---

### UVSD in R

```{r echo = F, tidy = FALSE, eval = F}
#Log likelihood for UVSD
uvsd <- function(dprime, crit, sigma, y, n) {
  {{p1 <- 1 - pnorm(crit, mean = dprime, sd = sigma)}}
  {{p2 <- 1 - pnorm(crit)}}
  - (dbinom(y["hits"], n["old"], p1, log = TRUE) +
    dbinom(y["fas"], n["new"], p2, log = TRUE))
}

#For both experimental conditions
nll.uvsd <- function(par, y, n){ #par = c(dprime1, dprime2, dprime3, crit, sigma)
  par[1:3] <- exp(par[1:3])
  par[5] <- exp(par[5])
  uvsd(par[1], par[4], par[5], y = y[, 1], n = n) +
    uvsd(par[2], par[4], par[5], y = y[, 2], n = n) +
    uvsd(par[3], par[4], par[5], y = y[, 3], n = n)
  }
```

```{r echo = T, tidy = FALSE, warning = F}
#Log likelihood for UVSD
uvsd <- function(dprime, crit, sigma, y, n) {
  {{p1 <- 1 - pnorm(crit, mean = dprime, sd = sigma)}}
  {{p2 <- 1 - pnorm(crit)}}
  - (dbinom(y["hits"], n["old"], p1, log = TRUE) +
    dbinom(y["fas"], n["new"], p2, log = TRUE))
}

# nested optimization
nll.uvsd <- function(par, y, n){ #par = c(crit, sigma)
  par[2] <- exp(par[2])
  out <- matrix(ncol = 2, nrow = ncol(y))
  for(i in 1:ncol(y)){
    tmp <- optimize(uvsd, interval = c(0,10)
                    , y = y[, i], n = n
                    , crit = par[1], sigma = par[2])
    out[i, 1] <- tmp$objective
    out[i, 2] <- tmp$minimum
  }
  return(sum(out[, 1], out[,2]))
}
```

---

### UVSD in R

```{r echo = T, warning = F}
g <- optim(par = c(crit = 0, sigma = log(1)), nll.uvsd, y = y, n = n)

crit <- g$par["crit"]
sigma <- exp(g$par["sigma"])
print(c(crit, sigma))
```

--

```{r echo = T, tidy = FALSE, warning = F}
out <- c()
for(i in 1:ncol(y)){
    tmp <- optimize(uvsd, interval = c(0,10)
                    , y = y[, i], n = n
                    , crit = crit, sigma = sigma)
    out[i] <- tmp$minimum
}
print(c(out, crit, sigma))
```

---
layout: false
class: inverse, middle, center

# Dealing with a hierarchical data structure

```{r, out.width = "50%", out.extra = 'id="zoom-margin"'}
include_graphics("src/hierarchy.jpeg")
```

---
class: my-one-page-font

### Nested models, nested optimization, now nested data?!

Data so far:

```{r, results='asis'}
rew <- c(10, 7, 5, 3, 1)
Reward1 <- paste(rew, "cents")
Reward2 <- paste(rev(rew), "cents")
Hit <- c(404, 348, 287, 251, 148)
Miss <- c(96, 152, 213, 249, 352)
FA <- c(301, 235, 183, 102, 20)
CR <- c(199, 265, 317, 398, 480)

tab <- cbind(Reward1, Reward2, Hit, Miss, FA, CR)
rownames(tab) <- paste("Cond", LETTERS[1:5])
colnames(tab)[1:2] <- c("Reward signal trial", "Reward noise trial")
knitr::kable(tab)
```

---
class: my-one-page-font

### Nested models, nested optimization, now nested data?!

Actual data:

```{r, results='asis'}
knitr::kable(cd)
```

---
class: my-one-page-font

### Nested models, nested optimization, now nested data?!

- It is common in experimental psychology to have a nested data structure.
- Each participant responds to several trials, typically across several experimental conditions.
--

- Also called within-subjects design (duh!).
--

- Typical analysis: Aggregate across participants.
--

- Is this appropriate for mathematical modeling?

---
class: my-one-page-font

### Fitting models to group data

- Sum data across participants, fit the model once.

.pull-left-50[

**Advantages**

- Simple.
- One (model comarison) result that can be interpreted immediately.

]

.pull-right-50[

**Disadvantages**

- Ignores individual differences.
- Masks individual differences.

]

---
class: my-one-page-font

### Fitting models to individual data

- If you have 30 participants, fit the model 30 times.

.pull-left-50[

**Advantages**

- Improved understanding of each person's processing structure.
- Allows for a more accurate assessment of robustness of model comparison results.

]

.pull-right-50[

**Disadvantages**

- Difficult to interpret (e.g., "HTM was preferred over SDM for 60% of participants.")
- May overstate individual differences.

]

---
class: my-one-page-font

### Accounting for the hierarchical structure

- Hierarchical modeling allows to optimally combine individual participant data.
- Models dependencies between observations within a person.
- Results in overall estimates across people *and* individual estimates.
- Much much more difficult.

---
class: my-one-page-font

### Accounting for the hierarchical structure

```{r, out.width = "100%", out.extra = 'id="zoom-margin"'}
include_graphics("src/stein.png")
```

.footnote[Efron & Morris, 1977]

---
class: my-one-page-font

### Accounting for the hierarchical structure

- More and more common for mathematical models.
- Introductory chapter: Rouder, Morey & Pratte (2017). *Bayesian hierarchical models of cognition.*

--

```{r, out.width = "100%", out.extra = 'id="zoom-margin"'}
include_graphics("src/hiersdt.png")
```

---
layout: false
class: inverse, middle, center

# Wrap up

---
class: my-one-page-font

### Wrap up

- Many optimization algorithms in R are pretty bad for many parameters.
- There are tricks to help (check convergence, transformation, nested optimization, ...).

--

Signal Detection models

- can be extended to be even more flexible.
- can be used for rating data.

--

Data in exp psy are 

- typically hierarchical.
- optimally modeled using hierarchical models.

At least we should fit models to individual participants' data.

---
class: inverse, middle, center

# :)

